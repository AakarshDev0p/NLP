{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AakarshDev0p/NLP/blob/main/2403A52262_NLP\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import spacy"
      ],
      "metadata": {
        "id": "6csJFERC8pRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------Tokenizing------------------\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4tot0boTlSDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Hospital = \"Cancer is being very chronic disease in the global level , it is been a world wide disease where lakhs of people are dying cause of cancer. \""
      ],
      "metadata": {
        "id": "NEscwAhh97xc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "word_tokenize(Hospital)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhEn4ESwBlO3",
        "outputId": "459fc0e2-a118-4e2c-9364-9e89e78f927e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Cancer',\n",
              " 'is',\n",
              " 'being',\n",
              " 'very',\n",
              " 'chronic',\n",
              " 'disease',\n",
              " 'in',\n",
              " 'the',\n",
              " 'global',\n",
              " 'level',\n",
              " ',',\n",
              " 'it',\n",
              " 'is',\n",
              " 'been',\n",
              " 'a',\n",
              " 'world',\n",
              " 'wide',\n",
              " 'disease',\n",
              " 'where',\n",
              " 'lakhs',\n",
              " 'of',\n",
              " 'people',\n",
              " 'are',\n",
              " 'dying',\n",
              " 'cause',\n",
              " 'of',\n",
              " 'cancer',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "sent_tokenize(Hospital)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUls3riVB4l_",
        "outputId": "7e79cbde-b03b-4465-9b47-2d1b273f3d64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Cancer is being very chronic disease in the global level , it is been a world wide disease where lakhs of people are dying cause of cancer.']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------Filterinhg Stop Words------------------"
      ],
      "metadata": {
        "id": "hRkrKAu5lfcv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjZzk2AxCA6X",
        "outputId": "24b36941-8a7d-4eaa-ce31-b7e4d44d6b07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words_in_quote = word_tokenize(Hospital)\n",
        "words_in_quote"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMrgiP4JCSSU",
        "outputId": "257eb6e9-dafc-4e9f-a333-0abfea1197c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Cancer',\n",
              " 'is',\n",
              " 'being',\n",
              " 'very',\n",
              " 'chronic',\n",
              " 'disease',\n",
              " 'in',\n",
              " 'the',\n",
              " 'global',\n",
              " 'level',\n",
              " ',',\n",
              " 'it',\n",
              " 'is',\n",
              " 'been',\n",
              " 'a',\n",
              " 'world',\n",
              " 'wide',\n",
              " 'disease',\n",
              " 'where',\n",
              " 'lakhs',\n",
              " 'of',\n",
              " 'people',\n",
              " 'are',\n",
              " 'dying',\n",
              " 'cause',\n",
              " 'of',\n",
              " 'cancer',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Hospital = \"Cancer is being very chronic disease in the global level , it is been a world wide disease where lakhs of people are dying cause of cancer. \"\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "words_in_quote = word_tokenize(Hospital)\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "filtered_list = []\n",
        "for word in words_in_quote:\n",
        "    if word.casefold() not in stop_words:\n",
        "        filtered_list.append(word)\n",
        "filtered_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-KnwxsOChuK",
        "outputId": "fecac52f-e82c-4b9b-a2c2-345ef6c0b250"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Cancer',\n",
              " 'chronic',\n",
              " 'disease',\n",
              " 'global',\n",
              " 'level',\n",
              " ',',\n",
              " 'world',\n",
              " 'wide',\n",
              " 'disease',\n",
              " 'lakhs',\n",
              " 'people',\n",
              " 'dying',\n",
              " 'cause',\n",
              " 'cancer',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------Stemming------------"
      ],
      "metadata": {
        "id": "oTDxnRG1lqrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "stemmer = PorterStemmer()\n",
        "words = word_tokenize(Hospital)\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "stemmed_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_uQzpcMC4_4",
        "outputId": "a90304fe-9ac3-49f3-9a8c-42bd2eeafbc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['cancer',\n",
              " 'is',\n",
              " 'be',\n",
              " 'veri',\n",
              " 'chronic',\n",
              " 'diseas',\n",
              " 'in',\n",
              " 'the',\n",
              " 'global',\n",
              " 'level',\n",
              " ',',\n",
              " 'it',\n",
              " 'is',\n",
              " 'been',\n",
              " 'a',\n",
              " 'world',\n",
              " 'wide',\n",
              " 'diseas',\n",
              " 'where',\n",
              " 'lakh',\n",
              " 'of',\n",
              " 'peopl',\n",
              " 'are',\n",
              " 'die',\n",
              " 'caus',\n",
              " 'of',\n",
              " 'cancer',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "Hospital = \"Cancer is being very chronic disease in the global level , it is been a world wide disease where lakhs of people are dying cause of cancer. \"\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "snowball = SnowballStemmer(language='english')\n",
        "words = word_tokenize(Hospital)\n",
        "for word in words:\n",
        "    print(word,\"---->\",snowball.stem(word))\n"
      ],
      "metadata": {
        "id": "80rsN5LjDjR6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b43b397-8c26-4003-9c01-f998f8065180"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cancer ----> cancer\n",
            "is ----> is\n",
            "being ----> be\n",
            "very ----> veri\n",
            "chronic ----> chronic\n",
            "disease ----> diseas\n",
            "in ----> in\n",
            "the ----> the\n",
            "global ----> global\n",
            "level ----> level\n",
            ", ----> ,\n",
            "it ----> it\n",
            "is ----> is\n",
            "been ----> been\n",
            "a ----> a\n",
            "world ----> world\n",
            "wide ----> wide\n",
            "disease ----> diseas\n",
            "where ----> where\n",
            "lakhs ----> lakh\n",
            "of ----> of\n",
            "people ----> peopl\n",
            "are ----> are\n",
            "dying ----> die\n",
            "cause ----> caus\n",
            "of ----> of\n",
            "cancer ----> cancer\n",
            ". ----> .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import LancasterStemmer\n",
        "Lanc = LancasterStemmer()\n",
        "words = word_tokenize(Hospital)\n",
        "for word in words:\n",
        "  print(word,\"---->\",Lanc.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udIuDDGsW-Q4",
        "outputId": "e07e2e51-e7bc-4ada-e6d7-c2728b9b1aa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cancer ----> cant\n",
            "is ----> is\n",
            "being ----> being\n",
            "very ----> very\n",
            "chronic ----> chronic\n",
            "disease ----> diseas\n",
            "in ----> in\n",
            "the ----> the\n",
            "global ----> glob\n",
            "level ----> level\n",
            ", ----> ,\n",
            "it ----> it\n",
            "is ----> is\n",
            "been ----> been\n",
            "a ----> a\n",
            "world ----> world\n",
            "wide ----> wid\n",
            "disease ----> diseas\n",
            "where ----> wher\n",
            "lakhs ----> lakh\n",
            "of ----> of\n",
            "people ----> peopl\n",
            "are ----> ar\n",
            "dying ----> dying\n",
            "cause ----> caus\n",
            "of ----> of\n",
            "cancer ----> cant\n",
            ". ----> .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RegexpStemmer\n",
        "regexp = RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
        "words = word_tokenize(Hospital)\n",
        "for word in words:\n",
        "  print(word,\"---->\",regexp.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvhDm-TyXP4n",
        "outputId": "9f857732-3dca-47f2-e7b6-6ea38403bfbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cancer ----> Cancer\n",
            "is ----> is\n",
            "being ----> be\n",
            "very ----> very\n",
            "chronic ----> chronic\n",
            "disease ----> diseas\n",
            "in ----> in\n",
            "the ----> the\n",
            "global ----> global\n",
            "level ----> level\n",
            ", ----> ,\n",
            "it ----> it\n",
            "is ----> is\n",
            "been ----> been\n",
            "a ----> a\n",
            "world ----> world\n",
            "wide ----> wid\n",
            "disease ----> diseas\n",
            "where ----> wher\n",
            "lakhs ----> lakh\n",
            "of ----> of\n",
            "people ----> peopl\n",
            "are ----> are\n",
            "dying ----> dy\n",
            "cause ----> caus\n",
            "of ----> of\n",
            "cancer ----> cancer\n",
            ". ----> .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "----------Lemmatization---------------"
      ],
      "metadata": {
        "id": "hImpSy3blyFD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Hospital = \"Cancer is being very chronic disease in the global level , it is been a world wide disease where lakhs of people are dying cause of cancer. \"\n",
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words = word_tokenize(Hospital)\n",
        "for word in words:\n",
        "  print(word,\"---->\",lemmatizer.lemmatize(word))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Icbso4EdXhLl",
        "outputId": "a8332964-7ce1-4bb5-e157-d176c7406b52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cancer ----> Cancer\n",
            "is ----> is\n",
            "being ----> being\n",
            "very ----> very\n",
            "chronic ----> chronic\n",
            "disease ----> disease\n",
            "in ----> in\n",
            "the ----> the\n",
            "global ----> global\n",
            "level ----> level\n",
            ", ----> ,\n",
            "it ----> it\n",
            "is ----> is\n",
            "been ----> been\n",
            "a ----> a\n",
            "world ----> world\n",
            "wide ----> wide\n",
            "disease ----> disease\n",
            "where ----> where\n",
            "lakhs ----> lakh\n",
            "of ----> of\n",
            "people ----> people\n",
            "are ----> are\n",
            "dying ----> dying\n",
            "cause ----> cause\n",
            "of ----> of\n",
            "cancer ----> cancer\n",
            ". ----> .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"worst\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "eRxM-OWSjbTn",
        "outputId": "7dd50141-b3d1-46d1-ceb6-aa9796a93059"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'worst'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"worst\",pos=\"a\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "YBPZkIGojhL9",
        "outputId": "f9f9a0ab-1a47-4ea8-a0cb-fa28f5928ded"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'bad'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "----------------Comaparision--------------"
      ],
      "metadata": {
        "id": "LhpdkWY1l2v6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer, RegexpStemmer, WordNetLemmatizer\n",
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "snowball = SnowballStemmer(language='english')\n",
        "regexp = RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "word_list = [\"friend\" , \"friendship\" , \"friends\" , \"friendships\"]\n",
        "print(\"{0:20}{1:20}{2:20}{3:30}{4:40}{5:50}\".format(\"Word\",\"Porter Stemmer\",\"Snowball Stemmer\",\"Lancaster Stemmer\",\"Regexp Stemmer\",\"WordNet Lemmatizer\"))\n",
        "print(\"-\"*160)\n",
        "for word in word_list:\n",
        "    print(\"{0:20}{1:20}{2:20}{3:30}{4:40}{5:50}\".format(word,porter.stem(word),snowball.stem(word),lancaster.stem(word),regexp.stem(word),lemmatizer.lemmatize(word)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_apaNR9Fjlzf",
        "outputId": "9d987172-b499-4dc4-c887-088723f0cb42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word                Porter Stemmer      Snowball Stemmer    Lancaster Stemmer             Regexp Stemmer                          WordNet Lemmatizer                                \n",
            "----------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "friend              friend              friend              friend                        friend                                  friend                                            \n",
            "friendship          friendship          friendship          friend                        friendship                              friendship                                        \n",
            "friends             friend              friend              friend                        friend                                  friend                                            \n",
            "friendships         friendship          friendship          friend                        friendship                              friendship                                        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "----------------PPT ASSIGNMENT-----------------\n",
        "\n",
        "write preprocessing output for :\n",
        "\n",
        "\"NLP models are transforming the world rapidly!â€"
      ],
      "metadata": {
        "id": "Y-2nXi-U7ctB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import spacy"
      ],
      "metadata": {
        "id": "oGb_EfNG7g9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subject=\"NLP models are transforming the world rapidly!\"\n",
        "\n"
      ],
      "metadata": {
        "id": "_cdh1FTu8Ga3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize\n",
        "word_tokenize(subject)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEGIVL0U8i91",
        "outputId": "f7f2dd02-eae1-4889-d8dd-e9b4409f197a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NLP', 'models', 'are', 'transforming', 'the', 'world', 'rapidly', '!']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "sent_tokenize(subject)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J57OVAgb8uNw",
        "outputId": "3b2f5c7f-b244-4689-9ddd-871c24c4b43f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NLP models are transforming the world rapidly!']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiPFZYnD9CO3",
        "outputId": "8167761d-17b1-42df-ec33-8ea9d5dfc56d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words_in_quote = word_tokenize(subject)\n",
        "words_in_quote"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDVQ0qGi9JoJ",
        "outputId": "a45f5d28-af4f-4f94-88bd-85640a45982e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NLP', 'models', 'are', 'transforming', 'the', 'world', 'rapidly', '!']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subject = \"NLP models are transforming the world rapidly!\"\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "words_in_quote = word_tokenize(subject)\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "filtered_list = []\n",
        "for word in words_in_quote:\n",
        "    if word.casefold() not in stop_words:\n",
        "        filtered_list.append(word)\n",
        "filtered_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DE83wut9SYl",
        "outputId": "369eb624-cd92-41ba-a220-af97e5cb4078"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NLP', 'models', 'transforming', 'world', 'rapidly', '!']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RegexpStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "regexp_stemmer = RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
        "words = word_tokenize(subject)\n",
        "stemmed_words = [regexp_stemmer.stem(word) for word in words]\n",
        "stemmed_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPHaX1px9fFz",
        "outputId": "b74bd477-dd76-40a7-a653-6a988740a0c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NLP', 'model', 'are', 'transform', 'the', 'world', 'rapidly', '!']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "subject = \"NLP models are transforming the world rapidly!\"\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "snowball = SnowballStemmer(language='english')\n",
        "words = word_tokenize(subject)\n",
        "for word in words:\n",
        "    print(word,\"---->\",snowball.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5nbh3679p2-",
        "outputId": "f8563735-7dbd-43ea-adaa-f3c3fe913422"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLP ----> nlp\n",
            "models ----> model\n",
            "are ----> are\n",
            "transforming ----> transform\n",
            "the ----> the\n",
            "world ----> world\n",
            "rapidly ----> rapid\n",
            "! ----> !\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import LancasterStemmer\n",
        "Lanc = LancasterStemmer()\n",
        "words = word_tokenize(subject)\n",
        "for word in words:\n",
        "  print(word,\"---->\",Lanc.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M34NWXLu9-pP",
        "outputId": "06bba667-ba96-4250-ef91-aaf265b7c38b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLP ----> nlp\n",
            "models ----> model\n",
            "are ----> ar\n",
            "transforming ----> transform\n",
            "the ----> the\n",
            "world ----> world\n",
            "rapidly ----> rapid\n",
            "! ----> !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RegexpStemmer\n",
        "regexp = RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
        "words = word_tokenize(name)\n",
        "for word in words:\n",
        "  print(word,\"---->\",regexp.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAEsAu7K-NLS",
        "outputId": "9e9b3aad-d112-40df-a019-96a3e9ec2ca1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLP ----> NLP\n",
            "models ----> model\n",
            "are ----> are\n",
            "transforming ----> transform\n",
            "the ----> the\n",
            "world ----> world\n",
            "rapidly ----> rapidly\n",
            "! ----> !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subject = \"NLP models are transforming the world rapidly!\"\n",
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words = word_tokenize(subject)\n",
        "for word in words:\n",
        "  print(word,\"---->\",lemmatizer.lemmatize(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCO8Ajbk-ZPZ",
        "outputId": "870379d0-3d92-4c0d-a923-e08f96f562ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLP ----> NLP\n",
            "models ----> model\n",
            "are ----> are\n",
            "transforming ----> transforming\n",
            "the ----> the\n",
            "world ----> world\n",
            "rapidly ----> rapidly\n",
            "! ----> !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"worst\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "vPA_boBx-hHC",
        "outputId": "48c83bcb-d9e7-4aae-db8c-9a21088f26d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'worst'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"worst\",pos=\"a\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "BAUoWHPl-wog",
        "outputId": "1f0b9fdc-c9a3-48b0-c658-6ac234f08676"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'bad'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Git Hub Assignment\n",
        "\n"
      ],
      "metadata": {
        "id": "qBe0vT3mAc5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import spacy"
      ],
      "metadata": {
        "id": "NN2coD2RAfs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SRUniversity= \"The SR University campus is located in Ananthasagar village of Hasanparthy Mandal in Warangal, Telangana, India.It is in 150 acres, with both separate hostel facilities for boys and girls. There is a huge central library along with Indias largest Technology Business Incubator (TBI) in tier 2 cities.\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7gEnCpLYAmx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "word_tokenize(SRUniversity)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tr6oJ0KKBYZQ",
        "outputId": "66c73d64-b08a-41d0-f6e3-e5087818d389"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'SR',\n",
              " 'University',\n",
              " 'campus',\n",
              " 'is',\n",
              " 'located',\n",
              " 'in',\n",
              " 'Ananthasagar',\n",
              " 'village',\n",
              " 'of',\n",
              " 'Hasanparthy',\n",
              " 'Mandal',\n",
              " 'in',\n",
              " 'Warangal',\n",
              " ',',\n",
              " 'Telangana',\n",
              " ',',\n",
              " 'India.It',\n",
              " 'is',\n",
              " 'in',\n",
              " '150',\n",
              " 'acres',\n",
              " ',',\n",
              " 'with',\n",
              " 'both',\n",
              " 'separate',\n",
              " 'hostel',\n",
              " 'facilities',\n",
              " 'for',\n",
              " 'boys',\n",
              " 'and',\n",
              " 'girls',\n",
              " '.',\n",
              " 'There',\n",
              " 'is',\n",
              " 'a',\n",
              " 'huge',\n",
              " 'central',\n",
              " 'library',\n",
              " 'along',\n",
              " 'with',\n",
              " 'Indias',\n",
              " 'largest',\n",
              " 'Technology',\n",
              " 'Business',\n",
              " 'Incubator',\n",
              " '(',\n",
              " 'TBI',\n",
              " ')',\n",
              " 'in',\n",
              " 'tier',\n",
              " '2',\n",
              " 'cities',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "sent_tokenize(SRUniversity)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njosmHiOBcqB",
        "outputId": "10205472-a241-498f-ce48-489ce1754e72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The SR University campus is located in Ananthasagar village of Hasanparthy Mandal in Warangal, Telangana, India.It is in 150 acres, with both separate hostel facilities for boys and girls.',\n",
              " 'There is a huge central library along with Indias largest Technology Business Incubator (TBI) in tier 2 cities.']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbpEQ00rBpYB",
        "outputId": "ff4e3d04-82f0-4bfa-9c12-7e576bcc4536"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "words_in_quote = word_tokenize(SRUniversity)\n",
        "words_in_quote"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvIgC-txBv-D",
        "outputId": "a2697641-86b0-4352-e0fb-1a2184bdd7d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'SR',\n",
              " 'University',\n",
              " 'campus',\n",
              " 'is',\n",
              " 'located',\n",
              " 'in',\n",
              " 'Ananthasagar',\n",
              " 'village',\n",
              " 'of',\n",
              " 'Hasanparthy',\n",
              " 'Mandal',\n",
              " 'in',\n",
              " 'Warangal',\n",
              " ',',\n",
              " 'Telangana',\n",
              " ',',\n",
              " 'India.It',\n",
              " 'is',\n",
              " 'in',\n",
              " '150',\n",
              " 'acres',\n",
              " ',',\n",
              " 'with',\n",
              " 'both',\n",
              " 'separate',\n",
              " 'hostel',\n",
              " 'facilities',\n",
              " 'for',\n",
              " 'boys',\n",
              " 'and',\n",
              " 'girls',\n",
              " '.',\n",
              " 'There',\n",
              " 'is',\n",
              " 'a',\n",
              " 'huge',\n",
              " 'central',\n",
              " 'library',\n",
              " 'along',\n",
              " 'with',\n",
              " 'Indias',\n",
              " 'largest',\n",
              " 'Technology',\n",
              " 'Business',\n",
              " 'Incubator',\n",
              " '(',\n",
              " 'TBI',\n",
              " ')',\n",
              " 'in',\n",
              " 'tier',\n",
              " '2',\n",
              " 'cities',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words(\"english\"))\n",
        "filtered_list = []\n",
        "for word in words_in_quote:\n",
        "  if word.casefold() not in stop_words:\n",
        "    filtered_list.append(word)\n",
        "filtered_list\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ji-JZSErB6WT",
        "outputId": "bda52b63-dffd-4f9f-9a0a-c002f84028c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['SR',\n",
              " 'University',\n",
              " 'campus',\n",
              " 'located',\n",
              " 'Ananthasagar',\n",
              " 'village',\n",
              " 'Hasanparthy',\n",
              " 'Mandal',\n",
              " 'Warangal',\n",
              " ',',\n",
              " 'Telangana',\n",
              " ',',\n",
              " 'India.It',\n",
              " '150',\n",
              " 'acres',\n",
              " ',',\n",
              " 'separate',\n",
              " 'hostel',\n",
              " 'facilities',\n",
              " 'boys',\n",
              " 'girls',\n",
              " '.',\n",
              " 'huge',\n",
              " 'central',\n",
              " 'library',\n",
              " 'along',\n",
              " 'Indias',\n",
              " 'largest',\n",
              " 'Technology',\n",
              " 'Business',\n",
              " 'Incubator',\n",
              " '(',\n",
              " 'TBI',\n",
              " ')',\n",
              " 'tier',\n",
              " '2',\n",
              " 'cities',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "stemmer = PorterStemmer()\n",
        "words = word_tokenize(SRUniversity)\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "stemmed_words\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c28xtofB9yr",
        "outputId": "d323af79-e4fe-420e-9d03-5d8b25e1e89d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the',\n",
              " 'sr',\n",
              " 'univers',\n",
              " 'campu',\n",
              " 'is',\n",
              " 'locat',\n",
              " 'in',\n",
              " 'ananthasagar',\n",
              " 'villag',\n",
              " 'of',\n",
              " 'hasanparthi',\n",
              " 'mandal',\n",
              " 'in',\n",
              " 'warang',\n",
              " ',',\n",
              " 'telangana',\n",
              " ',',\n",
              " 'india.it',\n",
              " 'is',\n",
              " 'in',\n",
              " '150',\n",
              " 'acr',\n",
              " ',',\n",
              " 'with',\n",
              " 'both',\n",
              " 'separ',\n",
              " 'hostel',\n",
              " 'facil',\n",
              " 'for',\n",
              " 'boy',\n",
              " 'and',\n",
              " 'girl',\n",
              " '.',\n",
              " 'there',\n",
              " 'is',\n",
              " 'a',\n",
              " 'huge',\n",
              " 'central',\n",
              " 'librari',\n",
              " 'along',\n",
              " 'with',\n",
              " 'india',\n",
              " 'largest',\n",
              " 'technolog',\n",
              " 'busi',\n",
              " 'incub',\n",
              " '(',\n",
              " 'tbi',\n",
              " ')',\n",
              " 'in',\n",
              " 'tier',\n",
              " '2',\n",
              " 'citi',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "snowball = SnowballStemmer(language='english')\n",
        "words = word_tokenize(SRUniversity)\n",
        "for word in words:\n",
        "    print(word,\"--->\",snowball.stem(word))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOqVVSWDCIAt",
        "outputId": "adda68a4-8a10-4ee9-f13b-719982f6a8db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The ---> the\n",
            "SR ---> sr\n",
            "University ---> univers\n",
            "campus ---> campus\n",
            "is ---> is\n",
            "located ---> locat\n",
            "in ---> in\n",
            "Ananthasagar ---> ananthasagar\n",
            "village ---> villag\n",
            "of ---> of\n",
            "Hasanparthy ---> hasanparthi\n",
            "Mandal ---> mandal\n",
            "in ---> in\n",
            "Warangal ---> warang\n",
            ", ---> ,\n",
            "Telangana ---> telangana\n",
            ", ---> ,\n",
            "India.It ---> india.it\n",
            "is ---> is\n",
            "in ---> in\n",
            "150 ---> 150\n",
            "acres ---> acr\n",
            ", ---> ,\n",
            "with ---> with\n",
            "both ---> both\n",
            "separate ---> separ\n",
            "hostel ---> hostel\n",
            "facilities ---> facil\n",
            "for ---> for\n",
            "boys ---> boy\n",
            "and ---> and\n",
            "girls ---> girl\n",
            ". ---> .\n",
            "There ---> there\n",
            "is ---> is\n",
            "a ---> a\n",
            "huge ---> huge\n",
            "central ---> central\n",
            "library ---> librari\n",
            "along ---> along\n",
            "with ---> with\n",
            "Indias ---> india\n",
            "largest ---> largest\n",
            "Technology ---> technolog\n",
            "Business ---> busi\n",
            "Incubator ---> incub\n",
            "( ---> (\n",
            "TBI ---> tbi\n",
            ") ---> )\n",
            "in ---> in\n",
            "tier ---> tier\n",
            "2 ---> 2\n",
            "cities ---> citi\n",
            ". ---> .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import LancasterStemmer\n",
        "Lanc = LancasterStemmer()\n",
        "words = word_tokenize(SRUniversity)\n",
        "for word in words:\n",
        "    print(word,\"--->\",Lanc.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esOpy7g2CKyH",
        "outputId": "98b095a7-b35d-4e4c-af3e-7dec0750d689"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The ---> the\n",
            "SR ---> sr\n",
            "University ---> univers\n",
            "campus ---> camp\n",
            "is ---> is\n",
            "located ---> loc\n",
            "in ---> in\n",
            "Ananthasagar ---> ananthasag\n",
            "village ---> vil\n",
            "of ---> of\n",
            "Hasanparthy ---> hasanparthy\n",
            "Mandal ---> mand\n",
            "in ---> in\n",
            "Warangal ---> warang\n",
            ", ---> ,\n",
            "Telangana ---> telangan\n",
            ", ---> ,\n",
            "India.It ---> india.it\n",
            "is ---> is\n",
            "in ---> in\n",
            "150 ---> 150\n",
            "acres ---> acr\n",
            ", ---> ,\n",
            "with ---> with\n",
            "both ---> both\n",
            "separate ---> sep\n",
            "hostel ---> hostel\n",
            "facilities ---> facil\n",
            "for ---> for\n",
            "boys ---> boy\n",
            "and ---> and\n",
            "girls ---> girl\n",
            ". ---> .\n",
            "There ---> ther\n",
            "is ---> is\n",
            "a ---> a\n",
            "huge ---> hug\n",
            "central ---> cent\n",
            "library ---> libr\n",
            "along ---> along\n",
            "with ---> with\n",
            "Indias ---> india\n",
            "largest ---> largest\n",
            "Technology ---> technolog\n",
            "Business ---> busy\n",
            "Incubator ---> incub\n",
            "( ---> (\n",
            "TBI ---> tbi\n",
            ") ---> )\n",
            "in ---> in\n",
            "tier ---> tier\n",
            "2 ---> 2\n",
            "cities ---> city\n",
            ". ---> .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RegexpStemmer\n",
        "regexp = RegexpStemmer('ing|s|e|able', min=4)\n",
        "words = word_tokenize(SRUniversity)\n",
        "for word in words:\n",
        "    print(word,\"--->\",Lanc.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yhPjZ4MCOez",
        "outputId": "bdbb8ce6-ebab-41cb-cd98-088efa252d14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The ---> the\n",
            "SR ---> sr\n",
            "University ---> univers\n",
            "campus ---> camp\n",
            "is ---> is\n",
            "located ---> loc\n",
            "in ---> in\n",
            "Ananthasagar ---> ananthasag\n",
            "village ---> vil\n",
            "of ---> of\n",
            "Hasanparthy ---> hasanparthy\n",
            "Mandal ---> mand\n",
            "in ---> in\n",
            "Warangal ---> warang\n",
            ", ---> ,\n",
            "Telangana ---> telangan\n",
            ", ---> ,\n",
            "India.It ---> india.it\n",
            "is ---> is\n",
            "in ---> in\n",
            "150 ---> 150\n",
            "acres ---> acr\n",
            ", ---> ,\n",
            "with ---> with\n",
            "both ---> both\n",
            "separate ---> sep\n",
            "hostel ---> hostel\n",
            "facilities ---> facil\n",
            "for ---> for\n",
            "boys ---> boy\n",
            "and ---> and\n",
            "girls ---> girl\n",
            ". ---> .\n",
            "There ---> ther\n",
            "is ---> is\n",
            "a ---> a\n",
            "huge ---> hug\n",
            "central ---> cent\n",
            "library ---> libr\n",
            "along ---> along\n",
            "with ---> with\n",
            "Indias ---> india\n",
            "largest ---> largest\n",
            "Technology ---> technolog\n",
            "Business ---> busy\n",
            "Incubator ---> incub\n",
            "( ---> (\n",
            "TBI ---> tbi\n",
            ") ---> )\n",
            "in ---> in\n",
            "tier ---> tier\n",
            "2 ---> 2\n",
            "cities ---> city\n",
            ". ---> .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('omw-1.4')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words = word_tokenize(SRUniversity)\n",
        "for word in words:\n",
        "    print(word,\"--->\",lemmatizer.lemmatize(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QR7OkX0ICRQ4",
        "outputId": "50fac783-3075-4726-a4ca-74f0c2c33ebd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The ---> The\n",
            "SR ---> SR\n",
            "University ---> University\n",
            "campus ---> campus\n",
            "is ---> is\n",
            "located ---> located\n",
            "in ---> in\n",
            "Ananthasagar ---> Ananthasagar\n",
            "village ---> village\n",
            "of ---> of\n",
            "Hasanparthy ---> Hasanparthy\n",
            "Mandal ---> Mandal\n",
            "in ---> in\n",
            "Warangal ---> Warangal\n",
            ", ---> ,\n",
            "Telangana ---> Telangana\n",
            ", ---> ,\n",
            "India.It ---> India.It\n",
            "is ---> is\n",
            "in ---> in\n",
            "150 ---> 150\n",
            "acres ---> acre\n",
            ", ---> ,\n",
            "with ---> with\n",
            "both ---> both\n",
            "separate ---> separate\n",
            "hostel ---> hostel\n",
            "facilities ---> facility\n",
            "for ---> for\n",
            "boys ---> boy\n",
            "and ---> and\n",
            "girls ---> girl\n",
            ". ---> .\n",
            "There ---> There\n",
            "is ---> is\n",
            "a ---> a\n",
            "huge ---> huge\n",
            "central ---> central\n",
            "library ---> library\n",
            "along ---> along\n",
            "with ---> with\n",
            "Indias ---> Indias\n",
            "largest ---> largest\n",
            "Technology ---> Technology\n",
            "Business ---> Business\n",
            "Incubator ---> Incubator\n",
            "( ---> (\n",
            "TBI ---> TBI\n",
            ") ---> )\n",
            "in ---> in\n",
            "tier ---> tier\n",
            "2 ---> 2\n",
            "cities ---> city\n",
            ". ---> .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"worst\", pos=\"a\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "g4IEDx7iC-5t",
        "outputId": "075e44cd-0e57-4be2-e2bb-f831ad4cbc07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'bad'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"worst\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "o43DFZUnDB5g",
        "outputId": "66812509-46b5-42b2-fdba-31c2a6a0bb36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'worst'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer, RegexpStemmer, WordNetLemmatizer\n",
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "snowball = SnowballStemmer(language='english')\n",
        "regexp = RegexpStemmer('ing|s|e|able', min=4)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\"]\n",
        "print(\"{0:20}{1:20}{2:20}{3:30}{4:40}{5:50}\".format(\"Word\",\"Porter Stemmer\",\"Snowball Stemmer\",\"Lancaster Stemmer\",'Regexp Stemmer','WordNetLemmatizer'))\n",
        "for word in word_list:\n",
        "    print(\"{0:20}{1:20}{2:20}{3:30}{4:40}{5:50}\".format(word,porter.stem(word),snowball.stem(word),lancaster.stem(word),regexp.stem(word),lemmatizer.lemmatize(word)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzOwrsnUDEoH",
        "outputId": "be967665-6efb-43a1-8c98-5c612a1426a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word                Porter Stemmer      Snowball Stemmer    Lancaster Stemmer             Regexp Stemmer                          WordNetLemmatizer                                 \n",
            "friend              friend              friend              friend                        frind                                   friend                                            \n",
            "friendship          friendship          friendship          friend                        frindhip                                friendship                                        \n",
            "friends             friend              friend              friend                        frind                                   friend                                            \n",
            "friendships         friendship          friendship          friend                        frindhip                                friendship                                        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sUaOt4biDSAg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}